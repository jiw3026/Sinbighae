{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpvhKGDbtQ6r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 호텔 리뷰 감성분석 (lstm)\n",
        "\n",
        "import pandas as pd\n",
        "hotelreview = pd.read_csv('train_set3.csv', encoding = 'cp949')\n",
        "hotelreview2 = pd.read_csv('test_set.csv')\n",
        "hotelreview2.head()\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "#import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#!pip install git+https://github.com/ssut/py-hanspell.git\n",
        "#from hanspell import spell_checker\n",
        "\n",
        "train_data = hotelreview\n",
        "test_data = hotelreview2\n",
        "\n",
        "train_data.head()\n",
        "test_data.head()\n",
        "train_data.tail()\n",
        "test_data.tail()\n",
        "print(len(train_data), len(test_data))\n",
        "\n",
        "train_data['reviews'].nunique(), train_data['label'].nunique()\n",
        "\n",
        "train_data.drop_duplicates(subset=['reviews'], inplace = True)\n",
        "print(len(train_data))\n",
        "\n",
        "train_data['label'].value_counts().plot(kind='bar')\n",
        "\n",
        "print(train_data.groupby('label').size().reset_index(name='count'))\n",
        "\n",
        "print(train_data.isnull().values.any())\n",
        "\n",
        "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ가-힣 ]\",\"\")\n",
        "train_data[:5]\n",
        "\n",
        "train_data['reviews'] = train_data['reviews'].str.replace('^ +', \"\")\n",
        "train_data['reviews'].replace('', np.nan, inplace = True)\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "train_data.loc[train_data.reviews.isnull()]\n",
        "\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "print(len(train_data))\n",
        "\n",
        "test_data.drop_duplicates(subset = ['reviews'], inplace = True)\n",
        "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ가-힣 ]\", \"\")\n",
        "test_data['reviews'] = test_data['reviews'].str.replace('^ +', \"\")\n",
        "test_data['reviews'].replace('', np.nan, inplace = True)\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "print(len(test_data))\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','걍','과','도','을','를','로','거','님',\n",
        "                           '때','으로','자','에','와','한','하다','이다','에서','수','고','이라','구','적','점','듯','그','에는',\n",
        "                           '있다', '하다', '있고', '나다', '들다','나','요','이에요',\n",
        "                   '해주다', '되어다', '제주', '제주도']\n",
        "\n",
        "okt = Okt()\n",
        "okt.morphs('깔끔하고 넓어서 좋았어요창으로 선산봉도 우도도 잘보이네요', stem = True)\n",
        "\n",
        "'''\n",
        "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지\"\n",
        "spelled_sent = spell_checker.check(sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)\n",
        "'''\n",
        "\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['reviews']):\n",
        "    #spelled_sent = spell_checker.check(sentence)\n",
        "    #hanspell_sent = spelled_sent.checked\n",
        "    tokenized_sentence = okt.morphs(sentence, norm = True, stem = True)\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
        "    X_train.append(stopwords_removed_sentence)\n",
        "\n",
        "print(X_train[:4])\n",
        "\n",
        "X_test = []\n",
        "for sentence in tqdm(test_data['reviews']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem = True)\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
        "    X_test.append(stopwords_removed_sentence)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index)\n",
        "rare_cnt = 0\n",
        "total_freq = 0\n",
        "rare_freq = 0\n",
        "\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "    \n",
        "    if (value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "        \n",
        "print('단어 집합의 크기: ', total_cnt)\n",
        "print('등장 빈도가 3번 미만인 단어 수: ', rare_cnt)\n",
        "print('단어 집합에서 희귀 단어의 비율: ', (rare_cnt / total_cnt))\n",
        "print('전체 등장 빈도에서 희귀 단어 등장 빈도 비율: ', (rare_freq / total_freq)*100)\n",
        "        \n",
        "\n",
        "# 희귀 단어 제거\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기: ', vocab_size)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(X_train[:4])\n",
        "\n",
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])\n",
        "\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train), len(y_train))\n",
        "\n",
        "print('리뷰의 최대 길이: ', max(len(review) for review in X_train))\n",
        "print('리뷰의 평균 길이: ', sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(review) for review in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "    count = 0\n",
        "    for sentence in nested_list:\n",
        "        if(len(sentence) <= max_len):\n",
        "            count = count + 1\n",
        "    print(f'전체 샘플 중 길이가 {max_len} 이하인 샘플의 비율: {(count / len(nested_list))*100}')\n",
        "\n",
        "max_len = 100\n",
        "below_threshold_len(max_len, X_train)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)\n",
        "\n",
        "vocab_size\n",
        "\n",
        "## LSTM\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train, 3)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test, 3)\n",
        "\n",
        "embedding_dim = 30\n",
        "hidden_units = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(3, activation = 'softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train_encoded, epochs=30, callbacks=[es,mc], batch_size=10, validation_split=0.2)\n",
        "\n",
        "\n",
        "loaded_model = load_model('best_model.h5')\n",
        "print(f'테스트 정확도: {loaded_model.evaluate(X_test, y_test_encoded)[1]}')\n",
        "\n",
        "def sentiment_predict(new_sentence):\n",
        "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ가-힣 ]','', new_sentence)\n",
        "    new_sentence = okt.morphs(new_sentence, norm=True, stem=True)\n",
        "    new_sentence = [word for word in new_sentence if not word in stopwords]\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
        "    pad_new = pad_sequences(encoded, maxlen = max_len)\n",
        "    score = loaded_model.predict(pad_new)\n",
        "    return score\n",
        "\n",
        "\n",
        "def reviewer():\n",
        "    myreview = input('리뷰를 입력하세요: ')\n",
        "    predict = sentiment_predict(myreview)\n",
        "\n",
        "    predict2 = predict.squeeze()\n",
        "    print(predict2)\n",
        "    if predict2.max() == predict2[0]:\n",
        "        print(f'재방문하지 않을 확률이 높습니다. {predict2.max()*100:.2f}%')\n",
        "    elif predict2.max() == predict2[1]:\n",
        "              print(f'중립리뷰일 확률이 높습니다. {predict2.max()*100:.2f}%')\n",
        "    else:\n",
        "        print(f'재방문할 확률이 높습니다. {predict2.max()*100:.2f}%')\n",
        "\n",
        "reviewer()\n",
        "\n",
        "\n",
        "# hanspell 사용 X, vocabsize=vocab_size, batch size=10, threshold=3"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFhm1hsWtRaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fClxZrImtRdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agX3-skNtRf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sJwTLYEtRiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7cICEBctRlY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}